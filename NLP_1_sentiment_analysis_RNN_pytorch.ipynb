{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31bdb1b0-3451-45fc-bc50-77d5a326e69b",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis using RNN\n",
    "#### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "373f8c10-317e-4fe4-99be-3ea69de568f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b24e240-f07f-4b97-b073-2c2a364af5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 12500/12500 [00:00<00:00, 81577.50it/s]\n",
      "100%|██████████████████████████████████| 12500/12500 [00:00<00:00, 85676.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews : 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# read sentiments and reviews data from the text files\n",
    "review_list = []\n",
    "label_list = []\n",
    "for label in ['pos', 'neg']:\n",
    "    for fname in tqdm(os.listdir(\n",
    "        f'./data/aclImdb/train/{label}/')):\n",
    "        if 'txt' not in fname:\n",
    "            continue\n",
    "        with open(os.path.join(f'./data/aclImdb/train/{label}/',\n",
    "                              fname), encoding=\"utf8\") as f:\n",
    "            review_list += [f.read()]\n",
    "            label_list += [label]\n",
    "print ('Number of reviews :', len(review_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "503d49c0-c357-4695-817f-033c2249350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 25000/25000 [00:00<00:00, 31828.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 334691), ('and', 162228), ('a', 161940), ('of', 145326), ('to', 135042), ('is', 106855), ('in', 93028), ('it', 77099), ('i', 75719), ('this', 75190)]\n"
     ]
    }
   ],
   "source": [
    "# pre-processing review text\n",
    "review_list = [review.lower() for review in review_list]\n",
    "review_list = [''.join([letter for letter in review\n",
    "                        if letter not in punctuation])\n",
    "                        for review in tqdm(review_list)]\n",
    "# accumulate all review texts together\n",
    "reviews_blob = ' '.join(review_list)\n",
    "# generate list of all words of all reviews\n",
    "review_words = reviews_blob.split()\n",
    "# get the word counts\n",
    "count_words = Counter(review_words)\n",
    "# sort words as per counts (decreasing order)\n",
    "total_review_words = len(review_words)\n",
    "sorted_review_words = count_words.most_common(total_review_words)\n",
    "print(sorted_review_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625831fc-0892-4b45-870f-820f23bda7c3",
   "metadata": {},
   "source": [
    "##### A word to integer dict. is created. Each word is given a unique integer for further embedding. Here , teh vocab. includes all teh words used in the training review set.\n",
    "##### example:  [i, like, this, movie] gets converted to [9, 38, 10, 17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e629f04-6ea3-44fc-b487-0ee65b2868ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1), ('and', 2), ('a', 3), ('of', 4), ('to', 5), ('is', 6), ('in', 7), ('it', 8), ('i', 9), ('this', 10)]\n",
      "vocab_to_token list length: 121364\n",
      "vocab_to_token length: 121364\n"
     ]
    }
   ],
   "source": [
    "# create word to integer (token) dictionary\n",
    "# in order to encode text as numbers\n",
    "vocab_to_token = {word:idx+1 for idx,\n",
    "                  (word, count) in enumerate(sorted_review_words)}\n",
    "print(list(vocab_to_token.items())[:10])\n",
    "print('vocab_to_token list length:',len(list(vocab_to_token.items())))\n",
    "print('vocab_to_token length:',len(vocab_to_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7f14e55-2f6c-4c3a-9cf1-1ea268b1d656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zentropa has much in common with the third man another noirlike film set among the rubble of postwar europe like ttm there is much inventive camera work there is an innocent american who gets emotionally involved with a woman he doesnt really understand and whose naivety is all the more striking in contrast with the nativesbr br but id have to say that the third man has a more wellcrafted storyline zentropa is a bit disjointed in this respect perhaps this is intentional it is presented as a dreamnightmare and making it too coherent would spoil the effect br br this movie is unrelentingly grimnoir in more than one sense one never sees the sun shine grim but intriguing and frightening\n",
      "\n",
      "[13147, 43, 72, 7, 1118, 16, 1, 837, 132, 153, 43770, 19, 272, 756, 1, 15187, 4, 6606, 2322, 38, 57550, 47, 6, 72, 4354, 384, 160, 47, 6, 33, 1309, 313, 36, 201, 2099, 560, 16, 3, 245, 26, 144, 62, 372, 2, 600, 14623, 6, 31, 1, 51, 3226, 7, 2212, 16, 1, 43771, 12, 18, 437, 25, 5, 129, 11, 1, 837, 132, 43, 3, 51, 10833, 742, 13147, 6, 3, 220, 4102, 7, 10, 1165, 369, 10, 6, 6805, 8, 6, 1313, 14, 3, 57551, 2, 244, 8, 99, 4125, 58, 2285, 1, 957, 12, 12, 10, 17, 6, 24234, 57552, 7, 51, 70, 28, 276, 28, 109, 1039, 1, 2895, 4103, 2601, 18, 1713, 2, 2518]\n"
     ]
    }
   ],
   "source": [
    "reviews_tokenized = []\n",
    "for review in review_list:\n",
    "    word_to_token = [vocab_to_token[word] for word in \n",
    "                     review.split()]\n",
    "    reviews_tokenized.append(word_to_token)\n",
    "print(review_list[0])\n",
    "print()\n",
    "print (reviews_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca218555-54fc-44a2-8d82-e4fa2446ac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 38, 10, 17]\n"
     ]
    }
   ],
   "source": [
    "### Optional for Understanding \n",
    "tmp_review='i like this movie'\n",
    "tmp_review_tokenised=[vocab_to_token[word] for word in tmp_review.split()]\n",
    "print(tmp_review_tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "930643f3-e7ec-43d3-b789-672ab0430c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode sentiments as 0 or 1\n",
    "encoded_label_list = [1 if label =='pos'\n",
    "                      else 0 for label in label_list]\n",
    "reviews_len = [len(review) for review in reviews_tokenized]\n",
    "reviews_tokenized = [reviews_tokenized[i] \n",
    "                     for i, l in enumerate(reviews_len)\n",
    "                     if l>0 ]\n",
    "encoded_label_list = np.array([encoded_label_list[i]\n",
    "                              for i, l in enumerate(reviews_len)\n",
    "                              if l> 0 ], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de176ed-77b1-4bff-b747-455670a2e534",
   "metadata": {},
   "source": [
    "##### Input dim to RNN should be consistent. But reviews will have variable length. Hence, padding/truncating is done to bring the sequence length to 512. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ccb5708-f111-4192-adb9-4aac29b30224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(reviews_tokenized, sequence_length):\n",
    "    ''' returns the tokenized review sequences padded with 0's or truncated to the sequence_length.\n",
    "    '''\n",
    "    padded_reviews = np.zeros((len(reviews_tokenized), sequence_length), dtype = int)\n",
    "    \n",
    "    for idx, review in enumerate(reviews_tokenized):\n",
    "        review_len = len(review)\n",
    "        \n",
    "        if review_len <= sequence_length:\n",
    "            zeroes = list(np.zeros(sequence_length-review_len))\n",
    "            new_sequence = zeroes+review\n",
    "        elif review_len > sequence_length:\n",
    "            new_sequence = review[0:sequence_length]\n",
    "        \n",
    "        padded_reviews[idx,:] = np.array(new_sequence)\n",
    "    \n",
    "    return padded_reviews\n",
    "\n",
    "sequence_length = 512\n",
    "padded_reviews = pad_sequence(reviews_tokenized=reviews_tokenized, sequence_length=sequence_length)\n",
    "#plt.hist(reviews_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "880c9679-b5a3-4a8c-8eae-591c2c3ba5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         9, 38, 10, 17]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Optional for Understanding \n",
    "### seq length is set to 20 here\n",
    "tmp_padded_review=pad_sequence(reviews_tokenized=[tmp_review_tokenised], sequence_length=20)\n",
    "(tmp_padded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a4d4542-8abf-4f66-ac5b-2edd86fd83ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = 0.75\n",
    "train_X = padded_reviews[:int(train_val_split*len(padded_reviews))]\n",
    "train_y = encoded_label_list[:int(train_val_split*len(padded_reviews))]\n",
    "validation_X = padded_reviews[int(train_val_split*len(padded_reviews)):]\n",
    "validation_y = encoded_label_list[int(train_val_split*len(padded_reviews)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29b67ad7-d771-47a7-a8bd-efa0d470c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate torch datasets\n",
    "train_dataset = TensorDataset(torch.from_numpy(train_X).to(device), torch.from_numpy(train_y).to(device))\n",
    "validation_dataset = TensorDataset(torch.from_numpy(validation_X).to(device), torch.from_numpy(validation_y).to(device))\n",
    "\n",
    "batch_size = 32\n",
    "# torch dataloaders (shuffle data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e16a466c-9b55-4bfd-bb5e-1fa4f0e25f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Input size:  torch.Size([32, 512])\n",
      "Example Input:\n",
      " tensor([[   25,    22,   107,  ...,   166,  1789,  4191],\n",
      "        [    0,     0,     0,  ...,     6,  3846,   161],\n",
      "        [    0,     0,     0,  ...,    55,   306,  2156],\n",
      "        ...,\n",
      "        [  437,   320,    42,  ...,   187,    54,   193],\n",
      "        [    0,     0,     0,  ...,  2146,     1,  1571],\n",
      "        [    0,     0,     0,  ..., 22996,    12,  1311]])\n",
      "\n",
      "Example Output size:  torch.Size([32])\n",
      "Example Output:\n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# get a batch of train data\n",
    "train_data_iter = iter(train_dataloader)\n",
    "X_example, y_example = next(train_data_iter)\n",
    "print('Example Input size: ', X_example.size()) # batch_size, seq_length\n",
    "print('Example Input:\\n', X_example)\n",
    "print()\n",
    "print('Example Output size: ', y_example.size()) # batch_size\n",
    "print('Example Output:\\n', y_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7437e-0105-40d5-b0a4-317b715736f8",
   "metadata": {},
   "source": [
    "##### Note: Embedding layer\n",
    "When an embedding layer is created, an embedding matrix is initialised with random vectors having dimensions of (num_embeddings(vocab_size), embedding_dim). This is basically our lookup table where our words are mapped to indexes.Given an input word or token, represented by its index in the vocabulary, you pass this index to the embedding layer which then looks up the corresponding row in the embedding matrix. The embedding vector is then extracted from the row as output which is of the dimension embedding_dim. During training, the embedding vectors are updated through backpropagation to minimize the loss. This means the vectors are adjusted to better represent the semantics and relationships between words for the given task here.\n",
    "\n",
    "##### like in time-series rnn, the input_dim can be (ex:t-5,t-4,t-3,t-2,t-1,t-0) & output can be (t+1). \n",
    "##### here we have seq lenth of 512 tokens. Each token is represented by dense-vector of size 100. Hence 512x100 will be processd one by one to get a binary output??  Basically, it processes word by word in a sequencial mannaer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71876d67-6a0b-4962-ad80-3544a0623494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dimension, embedding_dimension, \n",
    "                 hidden_dimension, output_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(input_dimension,\n",
    "                                        embedding_dimension)\n",
    "        self.rnn_layer = nn.RNN(embedding_dimension, \n",
    "                                hidden_dimension,\n",
    "                                num_layers=1)\n",
    "        self.fc_layer = nn.Linear(hidden_dimension,\n",
    "                                  output_dimension)\n",
    "    def forward(self, sequence):\n",
    "        # sequence shape = (sequence_length, batch_size)\n",
    "        embedding = self.embedding_layer(sequence)\n",
    "        # embedding shape = [sequence_length, batch_size, \n",
    "        #                    embedding_dimension]\n",
    "        output, hidden_state = self.rnn_layer(embedding)\n",
    "        # output shape = [sequence_length, batch_size, \n",
    "        #                 hidden_dimension]\n",
    "        # hidden_state shape = [1, batch_size, \n",
    "        #                      hidden_dimension]\n",
    "        final_output = self.fc_layer(\n",
    "            hidden_state[-1,:,:].squeeze(0))\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b680a32e-733a-49a9-b185-da33225cb99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# +1 to account for padding\n",
    "input_dimension = len(vocab_to_token)+1 \n",
    "embedding_dimension = 100\n",
    "hidden_dimension = 32\n",
    "output_dimension = 1\n",
    "rnn_model = RNN(input_dimension, embedding_dimension,\n",
    "                hidden_dimension, output_dimension)\n",
    "\n",
    "optim = torch.optim.Adam(rnn_model.parameters())\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "rnn_model = rnn_model.to(device)\n",
    "loss_func = loss_func.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f036528c-c926-4a87-a079-36e171ea56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Returns 0-1 accuracy for the given set \n",
    "    of predictions and ground truth\n",
    "    \"\"\"\n",
    "    # round predictions to either 0 or 1\n",
    "    rounded_predictions = \\\n",
    "        torch.round(torch.sigmoid(predictions))\n",
    "    # convert into float for division\n",
    "    success = (rounded_predictions == ground_truth).float()\n",
    "    accuracy = success.sum() / len(success)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83232803-42c0-4ca2-b9a0-5bca563b1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optim, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.train()\n",
    "    for sequence, sentiment in dataloader:\n",
    "        optim.zero_grad()\n",
    "        preds = model(sequence.T).squeeze()\n",
    "        loss_curr = loss_func(preds, sentiment)\n",
    "        accuracy_curr = accuracy_metric(preds, sentiment)\n",
    "        loss_curr.backward()\n",
    "        optim.step()\n",
    "        loss += loss_curr.item()\n",
    "        accuracy += accuracy_curr.item()\n",
    "    return loss/len(dataloader), accuracy/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82f34236-dd5b-4624-86d0-f0c25ff339ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sequence, sentiment in dataloader:\n",
    "            preds = model(sequence.T).squeeze()\n",
    "            loss_curr = loss_func(preds, sentiment)\n",
    "            accuracy_curr = accuracy_metric(preds, sentiment)\n",
    "            loss += loss_curr.item()\n",
    "            accuracy += accuracy_curr.item()\n",
    "    return loss/len(dataloader), accuracy/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "da68c281-0513-4167-8369-7bb92cc9fcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 1 | time elapsed: 66.82937407493591s\n",
      "training loss: 0.619 | training accuracy: 66.51%\n",
      "\tvalidation loss: 0.923 |  validation accuracy: 29.96%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "best_validation_loss = float('inf')\n",
    "for ep in range(num_epochs):\n",
    "    time_start = time.time()\n",
    "    training_loss, train_accuracy = train(rnn_model, \n",
    "                                          train_dataloader,\n",
    "                                          optim, loss_func)\n",
    "    validation_loss, validation_accuracy = validate(\n",
    "        rnn_model, validation_dataloader, loss_func)\n",
    "    time_end = time.time()\n",
    "    time_delta = time_end - time_start\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        torch.save(rnn_model.state_dict(), 'rnn_model.pt')\n",
    "    print(f'epoch number: {ep+1} | time elapsed: {time_delta}s')\n",
    "    print(f'training loss: {training_loss:.3f} | training accuracy: {train_accuracy*100:.2f}%')\n",
    "    print(f'\\tvalidation loss: {validation_loss:.3f} |  validation accuracy: {validation_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "253d8134-2188-484f-9c87-25b8db15ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_inference(model, sentence):\n",
    "    model.eval()\n",
    "    # text transformations\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([c for c in sentence\n",
    "                       if c not in punctuation])\n",
    "    tokenized = [vocab_to_token.get(token, 0)\n",
    "                 for token in sentence.split()]\n",
    "    tokenized = np.pad(tokenized,\n",
    "                       (512-len(tokenized), 0), 'constant')\n",
    "    # model inference\n",
    "    model_input = torch.LongTensor(tokenized).to(device)\n",
    "    model_input = model_input.unsqueeze(1)\n",
    "    pred = torch.sigmoid(model(model_input))\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "05c663d3-1700-424b-b71a-c79d76d2babf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3454607427120209\n",
      "0.3327416479587555\n",
      "0.302849143743515\n",
      "0.29525476694107056\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_inference(rnn_model,\n",
    "                          \"This film is horrible\"))\n",
    "print(sentiment_inference(rnn_model,\n",
    "                          \"Director tried too hard but \\\n",
    "                           this film is bad\"))\n",
    "print(sentiment_inference(rnn_model,\n",
    "                          \"This film will be houseful for weeks\"))\n",
    "print(sentiment_inference(rnn_model,\n",
    "                          \" I just really loved the movie\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
